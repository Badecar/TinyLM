# Training + model hyperparameters.
# Defaults here match the refined pipeline (elite dataset + larger model).
# To emulate the old TinyStories setup, see comments in this file and in configs/data.yaml.

# --- Training loop ---
context_size: 1024
batch_size: 64
learning_rate: 0.0006
max_steps: 80000
warmup_steps: 3000
grad_accum_steps: 2  # micro-batch accumulation e.g. (32 x 4 = 128 effective)
generation_interval: 100
# --- Model architecture ---
emb_dim: 1024
n_layers: 16
n_heads: 16
att_dim: 64
attention_impl: "sdpa" # "sdpa" (refined) or "manual" (old)
use_activation_checkpointing: true

#SMALL
# --- Training loop ---
# context_size: 512
# batch_size: 64
# learning_rate: 0.0006
# max_steps: 80000
# warmup_steps: 2000
# generation_interval: 1000
# # --- Model architecture ---
# emb_dim: 768
# n_layers: 12
# n_heads: 12
# att_dim: 64
# attention_impl: "sdpa" # "sdpa" (refined) or "manual" (old)
# use_activation_checkpointing: false

checkpoint_dir: "~/checkpoints"
resume_latest: false
# resume_from: version folder name under checkpoint_dir, e.g. "v1" -> loads "<checkpoint_dir>/v1/latest.pt"
resume_from:



# --- W&B ---
wandb_config: "configs/wandb.yaml"